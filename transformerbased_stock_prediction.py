# -*- coding: utf-8 -*-
"""transformerbased stock prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14xrddfg2M70IqD1ZTLzJXDQccI7VZ_8V
"""



# Commented out IPython magic to ensure Python compatibility.
from IPython import get_ipython
from IPython.display import display
# %load_ext cudf.pandas

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt

df = pd.read_excel('/content/Book1.xlsx')

print("DataFrame columns:", df.columns)

df.columns = df.columns.str.strip()
close_data = df["<CLOSE>"].values

scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(close_data.reshape(-1, 1)).flatten()

def create_sequences(data, seq_length):
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i : i + seq_length])
        y.append(data[i + seq_length])
    return np.array(X), np.array(y)

sequence_length = 10
X, y = create_sequences(scaled_data, sequence_length)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

class StockDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_dataset = StockDataset(X_train, y_train)
test_dataset = StockDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        seq_len = x.size(1)
        x = x + self.pe[:, :seq_len]
        return x

class StockTransformer(nn.Module):
    def __init__(self, seq_length, d_model=64, nhead=4, num_layers=1, dim_feedforward=128):
        super(StockTransformer, self).__init__()
        self.seq_length = seq_length
        self.d_model = d_model

        self.input_proj = nn.Linear(1, d_model)
        self.pos_encoder = PositionalEncoding(d_model, max_len=seq_length)

        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward,
            dropout=0.1
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc_out = nn.Linear(d_model, 1)

    def forward(self, x):
        x = x.unsqueeze(2)
        x = self.input_proj(x)
        x = self.pos_encoder(x)
        x = x.transpose(0, 1)
        x = self.transformer_encoder(x)
        x = x.mean(dim=0)
        output = self.fc_out(x)
        return output.squeeze(1)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = StockTransformer(seq_length=sequence_length).to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

num_epochs = 50
model.train()
print("Starting training...")
for epoch in range(num_epochs):
    epoch_loss = 0
    for batch_X, batch_y in train_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)
        optimizer.zero_grad()
        output = model(batch_X)
        loss = criterion(output, batch_y)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item() * batch_X.size(0)
    epoch_loss /= len(train_dataset)
    print(f"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.6f}")

model.eval()
test_predictions_scaled = []
test_actuals_scaled = []

print("\nEvaluating on test set...")
with torch.no_grad():
    for batch_X, batch_y in test_loader:
        batch_X = batch_X.to(device)
        output = model(batch_X)
        test_predictions_scaled.extend(output.cpu().numpy())
        test_actuals_scaled.extend(batch_y.cpu().numpy())

test_predictions = scaler.inverse_transform(np.array(test_predictions_scaled).reshape(-1, 1)).flatten()
test_actuals = scaler.inverse_transform(np.array(test_actuals_scaled).reshape(-1, 1)).flatten()

mse = mean_squared_error(test_actuals, test_predictions)
rmse = np.sqrt(mse)
mae = mean_absolute_error(test_actuals, test_predictions)
r2 = r2_score(test_actuals, test_predictions)

print("\nEvaluation Metrics on Test Set:")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"R-squared (R2): {r2:.4f}")

plt.figure(figsize=(14, 7))
plt.plot(test_actuals, label='Actual Value')
plt.plot(test_predictions, label='Predicted Value')
plt.title('Stock Price Prediction - Actual vs. Predicted (Test Set)')
plt.xlabel('Time Step')
plt.ylabel('CLOSE Price')
plt.legend()
plt.grid(True)
plt.show()

model.eval()
all_predictions_scaled = []
print("\nMaking predictions on entire dataset for plotting...")
with torch.no_grad():
    all_dataset = StockDataset(X, y)
    all_loader = DataLoader(all_dataset, batch_size=32, shuffle=False)

    for batch_X, batch_y in all_loader:
        batch_X = batch_X.to(device)
        output = model(batch_X)
        all_predictions_scaled.extend(output.cpu().numpy())

all_predictions = scaler.inverse_transform(np.array(all_predictions_scaled).reshape(-1, 1)).flatten()
original_close_data = scaler.inverse_transform(scaled_data.reshape(-1, 1)).flatten()

plt.figure(figsize=(14, 7))
plt.plot(original_close_data, label='Original Data')
plt.plot(np.arange(sequence_length, len(original_close_data)), all_predictions, label='Model Predictions')
plt.title('Stock Price - Original Data vs. Model Predictions')
plt.xlabel('Time Step')
plt.ylabel('CLOSE Price')
plt.legend()
plt.grid(True)
plt.show()

mse = mean_squared_error(test_actuals, test_predictions)
rmse = np.sqrt(mse)
mae = mean_absolute_error(test_actuals, test_predictions)
r2 = r2_score(test_actuals, test_predictions)

print("\nEvaluation Metrics on Test Set:")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"R-squared (R2): {r2:.4f}")

plt.figure(figsize=(14, 7))
plt.plot(test_actuals, label='Actual Value')
plt.plot(test_predictions, label='Predicted Value')
plt.title('Stock Price Prediction - Actual vs. Predicted (Test Set)')
plt.xlabel('Time Step')
plt.ylabel('CLOSE Price')
plt.legend()
plt.grid(True)
plt.show()